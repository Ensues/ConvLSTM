{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07850e48",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13389419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from PIL import Image\n",
    "import scipy.stats as ss\n",
    "import splitfolders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a768cd6",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510beac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Directory not found: \n",
      "Please update the VIDEO_DIR variable in this cell.\n",
      "WARNING: Directory not found: \n",
      "Please update the LABEL_DIR variable in this cell.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparanms\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "CHANNELS = 6\n",
    "FPS = 10\n",
    "DURATION = 3\n",
    "SEQ_LEN = FPS * DURATION\n",
    "\n",
    "# MVO Prediction Logic Mapping\n",
    "# FRONT: 0, LEFT: 1, RIGHT: 2\n",
    "def get_label_id(label_name):\n",
    "    mapping = {'front': 0, 'left': 1, 'right': 2}\n",
    "    return mapping.get(label_name.lower(), 0)\n",
    "\n",
    "VIDEO_DIR = r''\n",
    "LABEL_DIR = r''\n",
    "\n",
    "# Contains both and only video and label directories\n",
    "# Folder names are strictly \"videos\" and \"labels\"\n",
    "DATA_DIR = r'D:\\Thesis 2\\Thesis 2\\AIGD\\split folder' \n",
    "\n",
    "# Intent files\n",
    "VAL_POSITIONS = ''\n",
    "TEST_POSITIONS = ''\n",
    "\n",
    "# Incomplete_EluSEEdate_Dataset\n",
    "# Complete Final Training Datasets\n",
    "\n",
    "# Check if paths exist to avoid errors later\n",
    "if not os.path.exists(VIDEO_DIR):\n",
    "    print(f\"WARNING: Directory not found: {VIDEO_DIR}\")\n",
    "    print(\"Please update the VIDEO_DIR variable in this cell.\")\n",
    "\n",
    "if not os.path.exists(LABEL_DIR):\n",
    "    print(f\"WARNING: Directory not found: {LABEL_DIR}\")\n",
    "    print(\"Please update the LABEL_DIR variable in this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b63442",
   "metadata": {},
   "source": [
    "# conv_lstm_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca59000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    The Single Memory Unit of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The Observer of the video\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=True, bias=True, return_all_layers=False): # Changed default to batch_first=True\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        # input_tensor format: [Batch, Time, Channel, Height, Width]\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            # (Time, Batch, Channel, Height, Width) -> (Batch, Time, Channel, Height, Width)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Get Dimensions using the Correct Indices\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            # Loop over TIME (seq_len), not Batch\n",
    "            for t in range(seq_len):\n",
    "                # Slice the time dimension: [Batch, Channel, H, W]\n",
    "                # If layer_idx == 0, cur_layer_input is [B, T, C, H, W]\n",
    "                # If layer_idx > 0, cur_layer_input is [B, T, Hidden, H, W] (from previous layer stack)\n",
    "                \n",
    "                input_t = cur_layer_input[:, t, :, :, :]\n",
    "                \n",
    "                h, c = self.cell_list[layer_idx](input_tensor=input_t, cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            # Stack along Time dimension (dim=1 because we enforce batch_first internally now)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The Judge of the video.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, height, width,\n",
    "                 batch_first=True, bias=True, return_all_layers=False, num_classes=3):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "\n",
    "        # Ensure batch_first is passed correctly\n",
    "        self.convlstm = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers, \n",
    "                                 batch_first=batch_first, bias=bias, \n",
    "                                 return_all_layers=return_all_layers)\n",
    "\n",
    "        # Input to linear is: Hidden_Dim * H * W\n",
    "        self.linear = nn.Linear(hidden_dim[-1] * height * width, num_classes)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        x, _ = self.convlstm(input_tensor)\n",
    "\n",
    "        # x[0] shape is now guaranteed to be [Batch, Time, Hidden, H, W]\n",
    "        # We take the last time step: x[0][:, -1, :, :, :]\n",
    "        \n",
    "        last_time_step = x[0][:, -1, :, :, :]\n",
    "        \n",
    "        # Flatten starting from dimension 1 (Channels/Hidden)\n",
    "        flattened = torch.flatten(last_time_step, start_dim=1)\n",
    "\n",
    "        output = self.linear(flattened)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed131f",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e2939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVOVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This takes a 3-second video and turns it into\n",
    "    a 'data packet' for the AI to study.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_folder, label_folder, transforms=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.transforms = transforms\n",
    "\n",
    "        valid_video_files = []\n",
    "        valid_csv_files = []\n",
    "        try:\n",
    "            all_video_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "            for video_name in all_video_files:\n",
    "                video_path = os.path.join(video_folder, video_name)\n",
    "\n",
    "                csv_name = video_name.replace('.mp4', '.csv')\n",
    "                csv_path = os.path.join(label_folder, csv_name)\n",
    "                \"\"\"\n",
    "                # Check for standard labels first, then fallback to _A version\n",
    "                csv_name = video_name.replace('.mp4', '_labels.csv')\n",
    "                csv_path = os.path.join(label_folder, csv_name)\n",
    "\n",
    "                if not os.path.exists(csv_path):\n",
    "                    csv_name = video_name.replace('.mp4', '_labels_A.csv')\n",
    "                    csv_path = os.path.join(label_folder, csv_name)\n",
    "                \"\"\"\n",
    "\n",
    "                if os.path.exists(video_path) and os.path.exists(csv_path):\n",
    "                    valid_video_files.append(video_name)\n",
    "                    valid_csv_files.append(csv_name)\n",
    "                else:\n",
    "                    if not os.path.exists(video_path):\n",
    "                        print(f\"WARNING: Video file not found: {video_path}. Skipping.\")\n",
    "                    if not os.path.exists(csv_path):\n",
    "                        print(f\"WARNING: Label file not found for video {video_name}. Skipping.\")\n",
    "\n",
    "            self.video_files = valid_video_files\n",
    "            self.csv_files = valid_csv_files\n",
    "            print(f\"Initialized dataset with {len(self.video_files)} valid video-label pairs.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find folder {video_folder} or {label_folder}.\")\n",
    "            self.video_files = []\n",
    "            self.csv_files = []\n",
    "            \n",
    "        self.split_type = ''\n",
    "        self.positions = [] # If split_type == 'train', this would not be filled.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_files[idx]\n",
    "        video_path = os.path.join(self.video_folder, video_name)\n",
    "\n",
    "        #  Get the Label from the matching CSV file\n",
    "        csv_name = video_name.replace('.mp4', '.csv')\n",
    "        csv_path = os.path.join(self.label_folder, csv_name)\n",
    "        \n",
    "        # Read the label\n",
    "        # For a 3s clip, the label is the maximum turn label present in the last second (24 frames in CSV)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        label = self.labeler(df)\n",
    "\n",
    "        if self.split_type == 'TRAIN':\n",
    "            intent_position = self.get_intent_position()\n",
    "        else:\n",
    "            intent_position = self.positions[idx]\n",
    "\n",
    "        intent = self.get_intent(intent_position, df)\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        for i in range(30): \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # If a video is shorter than 3s, pad with a black frame\n",
    "                # Important: Ensure padding is the same shape/type as transformed frames\n",
    "                frame_tensor = torch.zeros((3, HEIGHT, WIDTH))\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                if self.transforms:\n",
    "                    frame = Image.fromarray(frame)\n",
    "                    frame_tensor = self.transforms(frame)\n",
    "                else:\n",
    "                    # Fallback if no transforms are provided\n",
    "                    frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "            # Create a tensor for the intent with the same spatial dimensions as the video frames\n",
    "            # Used for no intent\n",
    "            intent_torch = torch.zeros((3, HEIGHT, WIDTH))\n",
    "            # If intent exists, add intent in its intent position for 1 second (10 frames)  \n",
    "            if intent_position != -1 and intent_position <= i and (intent_position + 10) > i:\n",
    "                # Convert intent to one-hot vector by filling the specified channel with 1\n",
    "                intent_torch[intent, :, :] = 1\n",
    "\n",
    "            # Append the intent as a channel to the video frame\n",
    "            frame_tensor = torch.cat((frame_tensor, intent_torch), dim=0)\n",
    "            frames.append(frame_tensor) # Moved outside the 'else' to ensure we always have 30 frames\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "        # Convert list to a 5D tensor [30, 6, 128, 128]\n",
    "        video_tensor = torch.stack(frames, dim=0)\n",
    "\n",
    "        return video_tensor, torch.tensor(label).long()\n",
    "\n",
    "    def labeler(self, df):\n",
    "        df_lbl_count = []\n",
    "        # Logic to handle counts for classes 0, 1, and 2\n",
    "        # Added a check for the column name to prevent KeyErrors\n",
    "        col = 'label_id_corrected' if 'label_id_corrected' in df.columns else df.columns[-1]\n",
    "        counts = df[col].tail(24).value_counts()\n",
    "        \n",
    "        for i in range(0, 3):\n",
    "            df_lbl_count.append(counts.get(i, 0))\n",
    "\n",
    "        if df_lbl_count[0] == 24:\n",
    "            label = 0 # Front\n",
    "        elif df_lbl_count[1] > df_lbl_count[2]:\n",
    "            label = 1 # Left\n",
    "        elif df_lbl_count[1] < df_lbl_count[2]:\n",
    "            label = 2 # Right\n",
    "        else: \n",
    "            label = df[col].tail(12).mode()[0]\n",
    "\n",
    "        return label\n",
    "\n",
    "    def get_intent_position(self):\n",
    "        # 50% of the dataset have intent\n",
    "        if random.random() < 0.6:\n",
    "            # The time positions of the first 2 seconds (videos - 10 fps)\n",
    "            start_frame = 0\n",
    "            end_frame = 20\n",
    "            median = (start_frame + end_frame)/2\n",
    "            range_zero = np.arange(-median, median)\n",
    "\n",
    "            # Obtain the probability of selecting a timestamp using the adjacent 0.5 areas\n",
    "            smaller_range = range_zero - 0.5 \n",
    "            higher_range = range_zero + 0.5    \n",
    "\n",
    "            # Probability is the difference of the probability of higher range and lower range\n",
    "            probability = ss.norm.cdf(higher_range) - ss.norm.cdf(smaller_range)\n",
    "            \n",
    "            # Normalize the probabilities\n",
    "            # Each probability in probability range is divided by the sum of the probabilities in probability range\n",
    "            probability /= probability.sum()\n",
    "\n",
    "            # Select a timestamp based on the probabilities\n",
    "            range = np.arange(start_frame, end_frame)\n",
    "            intent_position = np.random.choice(range, p=probability)\n",
    "        else:\n",
    "            intent_position = -1\n",
    "        \n",
    "        return intent_position \n",
    "\n",
    "    def get_intent(self, intent_position, df):\n",
    "        # Check if the data has no intent\n",
    "        if intent_position != -1:\n",
    "            intent = self.labeler(df)\n",
    "        else:\n",
    "            intent = -1\n",
    "        return intent\n",
    "    \n",
    "    def class_counter(self):\n",
    "        label_counts = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "        for csv_file in self.csv_files:\n",
    "            csv_path = os.path.join(self.label_folder, csv_file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            label = self.labeler(df)\n",
    "            label_counts[label] += 1\n",
    "\n",
    "        return label_counts, sum(label_counts.values())\n",
    "\n",
    "    def set_split_type(self, type, len_dataset):\n",
    "        global VAL_POSITIONS\n",
    "        global TEST_POSITIONS\n",
    "        self.split_type = type\n",
    "\n",
    "        if self.split_type == 'VALIDATION':\n",
    "            if VAL_POSITIONS == '':\n",
    "                for _ in range(len_dataset):\n",
    "                    self.positions.append(self.get_intent_position())\n",
    "                np.save('val_intent_positions.npy', np.array(self.positions))\n",
    "                VAL_POSITIONS = 'val_intent_positions.npy'\n",
    "            else:\n",
    "                self.positions = list(np.load(VAL_POSITIONS))\n",
    "        elif self.split_type == 'TEST':\n",
    "            if TEST_POSITIONS == '':\n",
    "                for _ in range(len_dataset):\n",
    "                    self.positions.append(self.get_intent_position())\n",
    "                np.save('test_intent_positions.npy', np.array(self.positions))\n",
    "                TEST_POSITIONS = 'test_intent_positions.npy'\n",
    "            else:\n",
    "                self.positions = list(np.load(TEST_POSITIONS))\n",
    "\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0fa05",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc38885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 81 files [00:00, 85.96 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 48 valid video-label pairs.\n",
      "Initialized dataset with 16 valid video-label pairs.\n",
      "Initialized dataset with 17 valid video-label pairs.\n",
      "Data Split -> Train: 48 | Val: 16 | Test (Unused): 17\n",
      "Front class instances: 23 -> Front weight: 2.0\n",
      "Left class instances: 8 -> Left weight: 5.333333333333333\n",
      "Right class instances: 17 -> Right weight: 2.6666666666666665\n",
      "Training on cpu with 48 videos.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a7b6a339454814b476b784964327b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "BATCH = 5\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "SAVED_MODEL_PATH = \"best_convlstm.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setting a fixed random seed to ensure that\n",
    "# we get the exact same data split every time we run the script\n",
    "SEED = 8\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Model Parameters\n",
    "PARAMS = {\n",
    "    'input_dim': 6,\n",
    "    'hidden_dim': [64, 32],\n",
    "    'kernel_size': (3, 3),\n",
    "    'num_layers': 2,\n",
    "    'height': HEIGHT,\n",
    "    'width': WIDTH,\n",
    "    'num_classes': 3\n",
    "}\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.float().to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        running_loss += loss.item()\n",
    "        _, predictions = scores.max(1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return running_loss / len(loader), 100 * correct / total\n",
    "\n",
    "def main():\n",
    "    # Data Setup\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.Resize((HEIGHT, WIDTH)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Split the files in the directory into three directories: training (80%), validation (20%), and testing (20%)\n",
    "    splitfolders.ratio(DATA_DIR, output=\"output\", seed=8, ratio=(.6, .2, .2), group=\"sibling\",move=False, shuffle=True) \n",
    "\n",
    "    train_dir = os.path.join(\"output\", \"train\")\n",
    "    val_dir = os.path.join(\"output\", \"val\")\n",
    "    test_dir = os.path.join(\"output\", \"test\")\n",
    "\n",
    "    train_dir_vid = os.path.join(train_dir, \"videos\")\n",
    "    val_dir_vid = os.path.join(val_dir, \"videos\")\n",
    "    test_dir_vid = os.path.join(test_dir, \"videos\")\n",
    "\n",
    "    train_lbl_vid = os.path.join(train_dir, \"labels\")\n",
    "    val_lbl_vid = os.path.join(val_dir, \"labels\")\n",
    "    test_lbl_vid = os.path.join(test_dir, \"labels\")\n",
    "\n",
    "    train_dataset = MVOVideoDataset(train_dir_vid, train_lbl_vid, transforms=transforms_train)\n",
    "    val_dataset = MVOVideoDataset(val_dir_vid, val_lbl_vid, transforms=transforms_train)\n",
    "    test_dataset = MVOVideoDataset(test_dir_vid, test_lbl_vid, transforms=transforms_train)\n",
    "\n",
    "    print(f\"Data Split -> Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test (Unused): {len(test_dataset)}\")\n",
    "\n",
    "    train_dataset.set_split_type('TRAIN', len(train_dataset))\n",
    "    val_dataset.set_split_type('VALIDATION', len(val_dataset))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Calculate class instances for class weights\n",
    "    label_counts, total_count = train_dataset.class_counter()\n",
    "    \n",
    "    # Add 1 to avoid division by zero\n",
    "    front_weight = total_count / (label_counts[0] + 1) \n",
    "    left_weight = total_count / (label_counts[1] + 1)\n",
    "    right_weight = total_count / (label_counts[2] + 1)\n",
    "\n",
    "    print(f\"Front class instances: {label_counts[0]} -> Front weight: {front_weight}\")\n",
    "    print(f\"Left class instances: {label_counts[1]} -> Left weight: {left_weight}\")\n",
    "    print(f\"Right class instances: {label_counts[2]} -> Right weight: {right_weight}\")\n",
    "    \n",
    "    # Model Setup\n",
    "    model = ConvLSTMModel(\n",
    "        input_dim=PARAMS['input_dim'],\n",
    "        hidden_dim=PARAMS['hidden_dim'],\n",
    "        kernel_size=PARAMS['kernel_size'],\n",
    "        num_layers=PARAMS['num_layers'],\n",
    "        height=PARAMS['height'],\n",
    "        width=PARAMS['width'],\n",
    "        num_classes=PARAMS['num_classes']\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # CrossEntropyLoss is used to handle class imbalance\n",
    "    class_weights = torch.FloatTensor([front_weight,left_weight,right_weight]).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training Loop\n",
    "    best_acc = 0\n",
    "    print(f\"Training on {DEVICE} with {len(train_dataset)} videos.\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x = x.float().to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                scores = model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                val_correct += (preds == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save Best Model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), SAVED_MODEL_PATH)\n",
    "            print(f\"New best model saved! ({val_acc:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795daf4c",
   "metadata": {},
   "source": [
    "# tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    \"\"\"\n",
    "    It loads a trained model, feeds it unseen data,\n",
    "    and records how accurately and how fast the model makes decisions.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, device):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((HEIGHT, WIDTH))\n",
    "        ])\n",
    "\n",
    "        # Load the Architecture\n",
    "        self.model = ConvLSTMModel(\n",
    "            input_dim=6,\n",
    "            hidden_dim=[64, 32],\n",
    "            kernel_size=(3, 3),\n",
    "            num_layers=2,\n",
    "            height=HEIGHT,\n",
    "            width=WIDTH,\n",
    "            num_classes=3\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Load the Weights\n",
    "        self.load_weights()\n",
    "\n",
    "    def load_weights(self):\n",
    "        # Attempts to load the best model file and sets it to evaluation mode\n",
    "        if os.path.exists(self.model_path):\n",
    "            print(f\"Loading model from {self.model_path}...\")\n",
    "            self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file not found at {self.model_path}. Did you run train.py?\")\n",
    "\n",
    "    def test(self):\n",
    "        # The main evaluation loop.\n",
    "        print(\"Preparing Test Data...\")\n",
    "        \n",
    "        test_dir = os.path.join(\"output\", \"test\")\n",
    "        test_dir_vid = os.path.join(test_dir, \"videos\")\n",
    "        test_lbl_vid = os.path.join(test_dir, \"labels\")\n",
    "        test_dataset = MVOVideoDataset(test_dir_vid, test_lbl_vid, transforms=self.transforms)\n",
    "        test_dataset.set_split_type('TEST', len(test_dataset))\n",
    "        \n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        print(f\"Evaluating {len(test_dataset)} videos and measuring latency...\")\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        latencies = []\n",
    "\n",
    "        # Evaluation Loop\n",
    "        with torch.no_grad():\n",
    "            for i, (video_tensor, labels) in enumerate(tqdm(test_loader, leave=True)):\n",
    "                video_tensor = video_tensor.float().to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Latency Measurement Start\n",
    "                if self.device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.perf_counter() # Timer\n",
    "\n",
    "                outputs = self.model(video_tensor) # Forward pass (The Inference)\n",
    "\n",
    "                if self.device.type == 'cuda':\n",
    "                    torch.cuda.synchronize() # Wait for the GPU to finish the math\n",
    "\n",
    "                end_time = time.perf_counter()\n",
    "                # Latency Measurement End\n",
    "\n",
    "                # We skip the first 5 frames ('warm-up')\n",
    "                if i >= 5:\n",
    "                    latencies.append(end_time - start_time)\n",
    "\n",
    "                # Convert raw scores to the predicted class index (0, 1, or 2)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate Latency Stats\n",
    "        avg_latency_ms = np.mean(latencies) * 1000 if len(latencies) > 0 else 0\n",
    "        inf_fps = 1 / np.mean(latencies) if len(latencies) > 0 else 0\n",
    "\n",
    "        # Calculate and Print all results\n",
    "        self.calculate_metrics(all_labels, all_preds, avg_latency_ms, inf_fps)\n",
    "\n",
    "        # Save detailed logs to a CSV\n",
    "        self.save_results(all_labels, all_preds)\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred, avg_latency_ms, inf_fps):\n",
    "        # Computes statistical performance and prints the Final Report\n",
    "        print(f\"Avg Latency:        {avg_latency_ms:.2f} ms per video clip\")\n",
    "        print(f\"Inference Speed:    {inf_fps:.2f} clips per second\")\n",
    "        # Computes statistical performance and prints the Final Report\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"       FINAL PERFORMANCE REPORT       \")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        # Accuracy\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Overall Accuracy:   {acc*100:.2f}%\")\n",
    "\n",
    "        # Precision and Recall\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Precision:          {precision:.4f}\")\n",
    "        print(f\"Recall:             {recall:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        print(f\"Avg Latency:        {avg_latency_ms:.2f} ms per video clip\")\n",
    "        print(f\"Inference Speed:    {inf_fps:.2f} clips per second\")\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Detailed Class Report:\")\n",
    "        # Generates a table for Front(0), Left(1), Right(2)\n",
    "        print(classification_report(y_true, y_pred, target_names=['Front', 'Left', 'Right'], zero_division=0))\n",
    "\n",
    "    def save_results(self, y_true, y_pred):\n",
    "        # Creates a CSV to see exactly which videos failed\n",
    "        df = pd.DataFrame({\n",
    "            'Actual_Label': y_true,\n",
    "            'Predicted_Label': y_pred\n",
    "        })\n",
    "\n",
    "        # Map numbers back to words for readability\n",
    "        label_map = {0: 'Front', 1: 'Left', 2: 'Right'}\n",
    "        df['Actual_Text'] = df['Actual_Label'].map(label_map)\n",
    "        df['Predicted_Text'] = df['Predicted_Label'].map(label_map)\n",
    "\n",
    "        # Check if correct\n",
    "        df['Correct'] = df['Actual_Label'] == df['Predicted_Label']\n",
    "\n",
    "        save_path = \"test_results.csv\"\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"\\nDetailed predictions saved to '{save_path}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    MODEL_PATH = \"best_convlstm.pth\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Run the Tester\n",
    "    tester = Tester(MODEL_PATH, DEVICE)\n",
    "    tester.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
