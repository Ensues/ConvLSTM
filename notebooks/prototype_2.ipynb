{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07850e48",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13389419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a768cd6",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510beac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparanms\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "CHANNELS = 3\n",
    "FPS = 10\n",
    "DURATION = 3\n",
    "SEQ_LEN = FPS * DURATION\n",
    "\n",
    "# MVO Prediction Logic Mapping\n",
    "# FRONT: 0, LEFT: 1, RIGHT: 2\n",
    "def get_label_id(label_name):\n",
    "    mapping = {'front': 0, 'left': 1, 'right': 2}\n",
    "    return mapping.get(label_name.lower(), 0)\n",
    "\n",
    "VIDEO_DIR = r'C:\\Users\\ejans\\OneDrive\\Documents\\Thesis Stuff\\Eric Thesis Datasets\\test dataset\\videos'\n",
    "LABEL_DIR = r'C:\\Users\\ejans\\OneDrive\\Documents\\Thesis Stuff\\Eric Thesis Datasets\\test dataset\\labels'\n",
    "\n",
    "# Incomplete_EluSEEdate_Dataset\n",
    "# Complete Final Training Datasets\n",
    "\n",
    "# Check if paths exist to avoid errors later\n",
    "if not os.path.exists(VIDEO_DIR):\n",
    "    print(f\"WARNING: Directory not found: {VIDEO_DIR}\")\n",
    "    print(\"Please update the VIDEO_DIR variable in this cell.\")\n",
    "\n",
    "if not os.path.exists(LABEL_DIR):\n",
    "    print(f\"WARNING: Directory not found: {LABEL_DIR}\")\n",
    "    print(\"Please update the LABEL_DIR variable in this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b63442",
   "metadata": {},
   "source": [
    "# conv_lstm_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca59000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    The Single Memory Unit of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The Observer of the video\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=True, bias=True, return_all_layers=False): # Changed default to batch_first=True\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        # input_tensor format: [Batch, Time, Channel, Height, Width]\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            # (Time, Batch, Channel, Height, Width) -> (Batch, Time, Channel, Height, Width)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Get Dimensions using the Correct Indices\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            # Loop over TIME (seq_len), not Batch\n",
    "            for t in range(seq_len):\n",
    "                # Slice the time dimension: [Batch, Channel, H, W]\n",
    "                # If layer_idx == 0, cur_layer_input is [B, T, C, H, W]\n",
    "                # If layer_idx > 0, cur_layer_input is [B, T, Hidden, H, W] (from previous layer stack)\n",
    "                \n",
    "                input_t = cur_layer_input[:, t, :, :, :]\n",
    "                \n",
    "                h, c = self.cell_list[layer_idx](input_tensor=input_t, cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            # Stack along Time dimension (dim=1 because we enforce batch_first internally now)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The Judge of the video.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, height, width,\n",
    "                 batch_first=True, bias=True, return_all_layers=False, num_classes=3, dropout_rate=0.5):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "\n",
    "        # Ensure batch_first is passed correctly\n",
    "        self.convlstm = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers, \n",
    "                                 batch_first=batch_first, bias=bias, \n",
    "                                 return_all_layers=return_all_layers)\n",
    "\n",
    "        # Dropout for regularization (prevents overfitting)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Input to linear is: Hidden_Dim * H * W\n",
    "        self.linear = nn.Linear(hidden_dim[-1] * height * width, num_classes)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        x, _ = self.convlstm(input_tensor)\n",
    "\n",
    "        # x[0] shape is now guaranteed to be [Batch, Time, Hidden, H, W]\n",
    "        # We take the last time step: x[0][:, -1, :, :, :]\n",
    "        \n",
    "        last_time_step = x[0][:, -1, :, :, :]\n",
    "        \n",
    "        # Flatten starting from dimension 1 (Channels/Hidden)\n",
    "        flattened = torch.flatten(last_time_step, start_dim=1)\n",
    "\n",
    "        # Apply dropout for regularization (only active during training)\n",
    "        flattened = self.dropout(flattened)\n",
    "\n",
    "        output = self.linear(flattened)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed131f",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e2939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVOVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This takes a 3-second video and turns it into\n",
    "    a 'data packet' for the AI to study.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_folder, label_folder, transforms=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.transforms = transforms\n",
    "\n",
    "        valid_video_files = []\n",
    "        try:\n",
    "            all_video_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "            for video_name in all_video_files:\n",
    "                video_path = os.path.join(video_folder, video_name)\n",
    "\n",
    "                # Check for standard labels first, then fallback to _A version\n",
    "                csv_name = video_name.replace('.mp4', '_labels.csv')\n",
    "                csv_path = os.path.join(label_folder, csv_name)\n",
    "\n",
    "                if not os.path.exists(csv_path):\n",
    "                    csv_name = video_name.replace('.mp4', '_labels_A.csv')\n",
    "                    csv_path = os.path.join(label_folder, csv_name)\n",
    "\n",
    "                if os.path.exists(video_path) and os.path.exists(csv_path):\n",
    "                    valid_video_files.append(video_name)\n",
    "                else:\n",
    "                    if not os.path.exists(video_path):\n",
    "                        print(f\"WARNING: Video file not found: {video_path}. Skipping.\")\n",
    "                    if not os.path.exists(csv_path):\n",
    "                        print(f\"WARNING: Label file not found for video {video_name}. Skipping.\")\n",
    "\n",
    "            self.video_files = valid_video_files\n",
    "            print(f\"Initialized dataset with {len(self.video_files)} valid video-label pairs.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find folder {video_folder} or {label_folder}.\")\n",
    "            self.video_files = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_files[idx]\n",
    "        video_path = os.path.join(self.video_folder, video_name)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        for _ in range(30): \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # If a video is shorter than 3s, pad with a black frame\n",
    "                # Important: Ensure padding is the same shape/type as transformed frames\n",
    "                frame_tensor = torch.zeros((3, HEIGHT, WIDTH))\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                if self.transforms:\n",
    "                    frame = Image.fromarray(frame)\n",
    "                    frame_tensor = self.transforms(frame)\n",
    "                else:\n",
    "                    # Fallback if no transforms are provided\n",
    "                    frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "            frames.append(frame_tensor) # Moved outside the 'else' to ensure we always have 30 frames\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "        # Convert list to a 5D tensor [30, 3, 128, 128]\n",
    "        video_tensor = torch.stack(frames, dim=0)\n",
    "\n",
    "        # Get the Label from the matching CSV file\n",
    "        csv_name = video_name.replace('.mp4', '_labels.csv')\n",
    "        csv_path = os.path.join(self.label_folder, csv_name)\n",
    "        if not os.path.exists(csv_path):\n",
    "            csv_name = video_name.replace('.mp4', '_labels_A.csv')\n",
    "            csv_path = os.path.join(LABEL_DIR, csv_name)\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        label = self.labeler(df)\n",
    "\n",
    "        return video_tensor, torch.tensor(label).long()\n",
    "\n",
    "    def labeler(self, df):\n",
    "        df_lbl_count = []\n",
    "        # Logic to handle counts for classes 0, 1, and 2\n",
    "        # Added a check for the column name to prevent KeyErrors\n",
    "        col = 'label_id_corrected' if 'label_id_corrected' in df.columns else df.columns[-1]\n",
    "        counts = df[col].tail(24).value_counts()\n",
    "        \n",
    "        for i in range(0, 3):\n",
    "            df_lbl_count.append(counts.get(i, 0))\n",
    "\n",
    "        if df_lbl_count[0] == 24:\n",
    "            label = 0 # Front\n",
    "        elif df_lbl_count[1] > df_lbl_count[2]:\n",
    "            label = 1 # Left\n",
    "        elif df_lbl_count[1] < df_lbl_count[2]:\n",
    "            label = 2 # Right\n",
    "        else: \n",
    "            label = df[col].tail(12).mode()[0]\n",
    "\n",
    "        return label\n",
    "\n",
    "    def class_counter(self):\n",
    "        label_counts = {0: 0, 1: 0, 2: 0}\n",
    "        for video_name in self.video_files:\n",
    "            csv_name = video_name.replace('.mp4', '_labels.csv')\n",
    "            csv_path = os.path.join(self.label_folder, csv_name)\n",
    "            if not os.path.exists(csv_path):\n",
    "                csv_name = video_name.replace('.mp4', '_labels_A.csv')\n",
    "                csv_path = os.path.join(LABEL_DIR, csv_name)\n",
    "\n",
    "            df = pd.read_csv(csv_path)\n",
    "            label = self.labeler(df)\n",
    "            label_counts[label] += 1\n",
    "        return label_counts, sum(label_counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0fa05",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1829e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Label file not found for video E0000000076_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000089_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000149_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000180_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000260_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000265_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000334_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000335_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000343_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000361_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000365_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000371_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000376_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000382_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000387_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000400_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000404_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000406_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000411_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000431_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000488_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000494_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000501_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000504_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000513_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000530_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000539_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000545_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000550_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000567_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000570_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000602_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000618_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000632_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000639_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000644_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000655_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000659_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000694_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000700_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000701_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000705_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000724_A.mp4. Skipping.\n",
      "Initialized dataset with 257 valid video-label pairs.\n",
      "Data Split -> Train: 154 | Val: 51 | Test (Unused): 52\n",
      "Training on cpu with 154 videos.\n",
      "Early stopping enabled: patience=2, min_delta=0.01%\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad099dd8146d428da6e8d68b536a8814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0729 | Train Acc: 48.70% | Val Acc: 56.86%\n",
      "Avg Gradient Norm: 7.1528 (clipped at 1.0)\n",
      "Val Inference: 8616.20 ms/batch | 0.12 batches/sec\n",
      "✓ New best model saved! (56.86%)\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001929991e8d4616aafedf42fc11465b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0488 | Train Acc: 49.35% | Val Acc: 52.94%\n",
      "Avg Gradient Norm: 6.8623 (clipped at 1.0)\n",
      "Val Inference: 8685.15 ms/batch | 0.12 batches/sec\n",
      "No improvement for 1 epoch(s). Best: 56.86%\n",
      "\n",
      "✓ Training completed all 2 epochs.\n",
      "Final best validation accuracy: 56.86%\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "BATCH = 5\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "SAVED_MODEL_PATH = \"best_convlstm.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Early Stopping Configuration\n",
    "EARLY_STOP_PATIENCE = 2  # Stop if no improvement for 5 epochs\n",
    "MIN_DELTA = 0.01  # Minimum change to qualify as improvement (0.01%)\n",
    "\n",
    "# Setting a fixed random seed to ensure that\n",
    "# we get the exact same data split every time we run the script\n",
    "SEED = 8\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Model Parameters\n",
    "PARAMS = {\n",
    "    'input_dim': 3,\n",
    "    'hidden_dim': [64, 32],\n",
    "    'kernel_size': (3, 3),\n",
    "    'num_layers': 2,\n",
    "    'height': HEIGHT,\n",
    "    'width': WIDTH,\n",
    "    'num_classes': 3\n",
    "}\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_grad_norm = 0.0\n",
    "\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.float().to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping: Prevent exploding gradients in ConvLSTM\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        total_grad_norm += grad_norm.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        running_loss += loss.item()\n",
    "        _, predictions = scores.max(1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        loop.set_description(f\"Loss: {loss.item():.4f} | GradNorm: {grad_norm.item():.3f}\")\n",
    "\n",
    "    avg_grad_norm = total_grad_norm / len(loader)\n",
    "    return running_loss / len(loader), 100 * correct / total, avg_grad_norm\n",
    "\n",
    "def main():\n",
    "    # Data Setup\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.Resize((HEIGHT, WIDTH)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    full_dataset = MVOVideoDataset(VIDEO_DIR, LABEL_DIR, transforms=transforms_train)\n",
    "\n",
    "    # Train/Val/Test Split\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.6 * total_size)                  # 60% for Training\n",
    "    val_size = int(0.2 * total_size)                    # 20% for Validation\n",
    "    test_size = total_size - train_size - val_size      # Remaining 20% for Testing\n",
    "\n",
    "    # The generator with our fixed seed so the split is always the same\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "    # Split the dataset into three parts\n",
    "    train_dataset, val_dataset, _ = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    # Note: The last part is '_' because we don't touch the test set in train.py\n",
    "\n",
    "    print(f\"Data Split -> Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test (Unused): {test_size}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model Setup\n",
    "    model = ConvLSTMModel(\n",
    "        input_dim=PARAMS['input_dim'],\n",
    "        hidden_dim=PARAMS['hidden_dim'],\n",
    "        kernel_size=PARAMS['kernel_size'],\n",
    "        num_layers=PARAMS['num_layers'],\n",
    "        height=PARAMS['height'],\n",
    "        width=PARAMS['width'],\n",
    "        num_classes=PARAMS['num_classes']\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training Loop with Early Stopping\n",
    "    best_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    print(f\"Training on {DEVICE} with {len(train_dataset)} videos.\")\n",
    "    print(f\"Early stopping enabled: patience={EARLY_STOP_PATIENCE}, min_delta={MIN_DELTA}%\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        train_loss, train_acc, avg_grad_norm = train_one_epoch(model, train_loader, criterion, optimizer, max_grad_norm=1.0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_latencies = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, y) in enumerate(val_loader):\n",
    "                x = x.float().to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                \n",
    "                # Measure inference time\n",
    "                if DEVICE.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.perf_counter()\n",
    "                scores = model(x)\n",
    "                \n",
    "                if DEVICE.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                end_time = time.perf_counter()\n",
    "                \n",
    "                # Skip first batch for warm-up\n",
    "                if batch_idx >= 1:\n",
    "                    val_latencies.append(end_time - start_time)\n",
    "                \n",
    "                _, preds = scores.max(1)\n",
    "                val_correct += (preds == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_latency_ms = (np.mean(val_latencies) * 1000) if len(val_latencies) > 0 else 0\n",
    "        val_throughput = (1 / np.mean(val_latencies)) if len(val_latencies) > 0 else 0\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Avg Gradient Norm: {avg_grad_norm:.4f} (clipped at 1.0)\")\n",
    "        print(f\"Val Inference: {avg_val_latency_ms:.2f} ms/batch | {val_throughput:.2f} batches/sec\")\n",
    "\n",
    "        # Save Best Model & Early Stopping Check\n",
    "        if val_acc > best_acc + MIN_DELTA:\n",
    "            best_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), SAVED_MODEL_PATH)\n",
    "            print(f\"✓ New best model saved! ({val_acc:.2f}%)\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s). Best: {best_acc:.2f}%\")\n",
    "            \n",
    "            if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"\\n⚠ Early stopping triggered! No improvement for {EARLY_STOP_PATIENCE} epochs.\")\n",
    "                print(f\"Best validation accuracy: {best_acc:.2f}%\")\n",
    "                early_stop = True\n",
    "                break\n",
    "    \n",
    "    if not early_stop:\n",
    "        print(f\"\\n✓ Training completed all {NUM_EPOCHS} epochs.\")\n",
    "    print(f\"Final best validation accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795daf4c",
   "metadata": {},
   "source": [
    "# tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ea4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from best_convlstm.pth...\n",
      "Preparing Test Data...\n",
      "WARNING: Label file not found for video E0000000076_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000089_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000149_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000180_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000260_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000265_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000334_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000335_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000343_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000361_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000365_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000371_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000376_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000382_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000387_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000400_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000404_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000406_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000411_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000431_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000488_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000494_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000501_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000504_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000513_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000530_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000539_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000545_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000550_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000567_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000570_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000602_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000618_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000632_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000639_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000644_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000655_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000659_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000694_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000700_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000701_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000705_A.mp4. Skipping.\n",
      "WARNING: Label file not found for video E0000000724_A.mp4. Skipping.\n",
      "Initialized dataset with 257 valid video-label pairs.\n",
      "Evaluating 52 videos and measuring latency...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0678636c52d54e60b49c1118513dc335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Latency:        2302.72 ms per video clip\n",
      "Inference Speed:    0.43 clips per second\n",
      "\n",
      "----------------------------------------\n",
      "       FINAL PERFORMANCE REPORT       \n",
      "----------------------------------------\n",
      "Overall Accuracy:   46.15%\n",
      "Precision:          0.3332\n",
      "Recall:             0.4615\n",
      "----------------------------------------\n",
      "Avg Latency:        2302.72 ms per video clip\n",
      "Inference Speed:    0.43 clips per second\n",
      "----------------------------------------\n",
      "Detailed Class Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Front       0.51      0.79      0.62        24\n",
      "        Left       0.00      0.00      0.00        13\n",
      "       Right       0.33      0.33      0.33        15\n",
      "\n",
      "    accuracy                           0.46        52\n",
      "   macro avg       0.28      0.38      0.32        52\n",
      "weighted avg       0.33      0.46      0.38        52\n",
      "\n",
      "\n",
      "Detailed predictions saved to 'test_results.csv'\n"
     ]
    }
   ],
   "source": [
    "class Tester:\n",
    "    \"\"\"\n",
    "    It loads a trained model, feeds it unseen data,\n",
    "    and records how accurately and how fast the model makes decisions.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, device):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((HEIGHT, WIDTH))\n",
    "        ])\n",
    "\n",
    "        # Load the Architecture\n",
    "        self.model = ConvLSTMModel(\n",
    "            input_dim=3,\n",
    "            hidden_dim=[64, 32],\n",
    "            kernel_size=(3, 3),\n",
    "            num_layers=2,\n",
    "            height=HEIGHT,\n",
    "            width=WIDTH,\n",
    "            num_classes=3,\n",
    "            dropout_rate=0.5  # Dropout not applied during eval mode\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Load the Weights\n",
    "        self.load_weights()\n",
    "\n",
    "    def load_weights(self):\n",
    "        # Attempts to load the best model file and sets it to evaluation mode\n",
    "        if os.path.exists(self.model_path):\n",
    "            print(f\"Loading model from {self.model_path}...\")\n",
    "            self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file not found at {self.model_path}. Did you run train.py?\")\n",
    "\n",
    "    def test(self):\n",
    "        # The main evaluation loop.\n",
    "        print(\"Preparing Test Data...\")\n",
    "        full_dataset = MVOVideoDataset(VIDEO_DIR, LABEL_DIR, transforms=self.transforms)\n",
    "\n",
    "        # Re-creating the 60/20/20 split\n",
    "        SEED = 8\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.6 * total_size)\n",
    "        val_size = int(0.2 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "\n",
    "        generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "        # Split, but this time we only care about the LAST chunk (test_dataset)\n",
    "        _, _, test_dataset = random_split(\n",
    "            full_dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=generator\n",
    "        )\n",
    "        # Note that the first two are \"_\" again because wwe do not need them\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        print(f\"Evaluating {len(test_dataset)} videos and measuring latency...\")\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        latencies = []\n",
    "\n",
    "        # Evaluation Loop\n",
    "        with torch.no_grad():\n",
    "            for i, (video_tensor, labels) in enumerate(tqdm(test_loader, leave=True)):\n",
    "                video_tensor = video_tensor.float().to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Latency Measurement Start\n",
    "                if self.device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                start_time = time.perf_counter() # Timer\n",
    "\n",
    "                outputs = self.model(video_tensor) # Forward pass (The Inference)\n",
    "\n",
    "                if self.device.type == 'cuda':\n",
    "                    torch.cuda.synchronize() # Wait for the GPU to finish the math\n",
    "\n",
    "                end_time = time.perf_counter()\n",
    "                # Latency Measurement End\n",
    "\n",
    "                # We skip the first 5 frames ('warm-up')\n",
    "                if i >= 5:\n",
    "                    latencies.append(end_time - start_time)\n",
    "\n",
    "                # Convert raw scores to the predicted class index (0, 1, or 2)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate Latency Stats\n",
    "        avg_latency_ms = np.mean(latencies) * 1000 if len(latencies) > 0 else 0\n",
    "        inf_fps = 1 / np.mean(latencies) if len(latencies) > 0 else 0\n",
    "\n",
    "        # Calculate and Print all results\n",
    "        self.calculate_metrics(all_labels, all_preds, avg_latency_ms, inf_fps)\n",
    "\n",
    "        # Save detailed logs to a CSV\n",
    "        self.save_results(all_labels, all_preds)\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred, avg_latency_ms, inf_fps):\n",
    "        # Computes statistical performance and prints the Final Report\n",
    "        print(f\"Avg Latency:        {avg_latency_ms:.2f} ms per video clip\")\n",
    "        print(f\"Inference Speed:    {inf_fps:.2f} clips per second\")\n",
    "        # Computes statistical performance and prints the Final Report\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"       FINAL PERFORMANCE REPORT       \")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        # Accuracy\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Overall Accuracy:   {acc*100:.2f}%\")\n",
    "\n",
    "        # Precision and Recall\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Precision:          {precision:.4f}\")\n",
    "        print(f\"Recall:             {recall:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        print(f\"Avg Latency:        {avg_latency_ms:.2f} ms per video clip\")\n",
    "        print(f\"Inference Speed:    {inf_fps:.2f} clips per second\")\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Detailed Class Report:\")\n",
    "        # Generates a table for Front(0), Left(1), Right(2)\n",
    "        print(classification_report(y_true, y_pred, target_names=['Front', 'Left', 'Right'], zero_division=0))\n",
    "\n",
    "    def save_results(self, y_true, y_pred):\n",
    "        # Creates a CSV to see exactly which videos failed\n",
    "        df = pd.DataFrame({\n",
    "            'Actual_Label': y_true,\n",
    "            'Predicted_Label': y_pred\n",
    "        })\n",
    "\n",
    "        # Map numbers back to words for readability\n",
    "        label_map = {0: 'Front', 1: 'Left', 2: 'Right'}\n",
    "        df['Actual_Text'] = df['Actual_Label'].map(label_map)\n",
    "        df['Predicted_Text'] = df['Predicted_Label'].map(label_map)\n",
    "\n",
    "        # Check if correct\n",
    "        df['Correct'] = df['Actual_Label'] == df['Predicted_Label']\n",
    "\n",
    "        save_path = \"test_results.csv\"\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"\\nDetailed predictions saved to '{save_path}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    MODEL_PATH = \"best_convlstm.pth\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Run the Tester\n",
    "    tester = Tester(MODEL_PATH, DEVICE)\n",
    "    tester.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
