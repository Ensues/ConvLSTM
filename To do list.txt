Eric & Hannah To do list:

Medium Effort, High Reward:
- Mixed precision training: Use torch.cuda.amp for faster training (needs GPU, we have none right now, wag na muna)
- Tensorboard logging: Track loss curves, accuracy, learning rate 5
- Batch normalization: Add after ConvLSTM layers for training stability 3
- Validation metrics per class: Track precision/recall per class during training 8

Big Projects (High Impact but Time-Intensive):
- Video preprocessing: Pre-extract and cache frames to disk 4
- TorchScript compilation: torch.jit.script() for faster inference 9
- ONNX export: Run on optimized runtime 10

Nice-to-Have:
- Resume training: Save/load optimizer state and epoch number 7
- Checkpointing: Save model every N epochs, not just best 6

Eric Finished List:
- Gradient clipping: Prevent exploding gradients
- Early stopping: Stop training when validation stops improving
- Dropout layers: Add dropout in the linear layer to prevent overfitting
- Track inference time during validation
- Learning rate scheduling: Reduce LR when validation plateaus (ReduceLROnPlateau)
- Delete intermediate tensors: Use del and clear unnecessary variables
- Gradient accumulation: Simulate larger batches with less memory